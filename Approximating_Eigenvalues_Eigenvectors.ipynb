{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 2820 - LINEAR ALGEBRA - SPRING 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you fill in any place that says `CODE SOLUTION HERE` or \"CODE SOLUTION HERE\", as well as your NAMES below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES = \"Julia Aronow, Jules Novoa\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PROJECT (Option 1):  Approximating Eigenvalues and Eigenvectors and Applications in Discrete Dynamical Systems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a Jupyter notebook for the CU Linear Algebra Final Project. \n",
    "## Professor Divya E. Vernerey and Arjun Peroor\n",
    "## Spring 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first section we look at three methods that can be used to approximate the eigenvalues of a matrix $A$.  Although it is possible to find the exact eigenvalues for small matrices, the approach is impractical for larger matrices. Read Lay section 5.8 for more detailed information.\n",
    "\n",
    "Most introductory textbooks demonstrate a direct way to compute eigenvalues of an $n\\times n$ matrix $A$ by computing roots of an associated $n$th degree polynomial, known as the *characteristic polynomial*.  For example, suppose $A$ is a $2\\times 2$ matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rr} a & b  \\\\ c & d \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The eigenvalues of $A$ are solutions to the quadratic equation $\\lambda^2 - (a+d)\\lambda + ad-bc = 0$, which can be written explicitly in terms of $a$, $b$, $c$, and $d$ using the quadratic formula.  The challenges with larger matrices are that the polynomial is more difficult to construct, and the roots cannot be easily found with a formula.\n",
    "\n",
    "The algorithms we describe in this section are iterative methods.  They generate a sequence of vectors $\\{X^{(1)}, X^{(2)}, X^{(3)}, ... \\}$ that approach a true eigenvector of the matrix under consideration.  An approximation of the corresponding eigenvalue can then be computed by multiplying the approximate eigenvector by $A$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power method (method 1)\n",
    "\n",
    "The first algorithm we introduce for approximating eigenvalues is known as the **Power Method**.  This method generates a sequence of vectors by repeated matrix multiplication.  Under suitable conditions, the sequence of vectors approaches the eigenvector associated with the eigenvalue that is largest in absolute value.    \n",
    "\n",
    "For the simplest explanation, suppose that $A$ is an $n\\times n$ diagonalizable matrix with eigenvectors $\\{V_1, V_2, ... V_n\\}$, and that $\\lambda_1$ is the eigenvalue of $A$ that is largest in absolute value.  To begin the Power Method, we choose any nonzero vector and label it $X^{(0)}$.  We can express $X^{(0)}$  as a linear combination of the eigenvectors since they form a basis for $\\mathbb{R}^n$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(0)} = c_1V_1 + c_2V_2 + ... c_nV_n\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We now form a sequence of vectors $X^{(1)}$, $X^{(2)}$, $X^{(3)}$, ..., by setting $X^{(m)}= AX^{(m-1)}$.  Each of these vectors is also easly expressed in terms of the eigenvectors.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "X^{(1)} = AX^{(0)} & = & c_1AV_1 + c_2AV_2 + ... c_nAV_n \\\\\n",
    "                   & = & c_1\\lambda_1V_1 + c_2\\lambda_2V_2 + ... c_n\\lambda_nV_n \\\\\n",
    "X^{(2)} = AX^{(1)} & = & c_1\\lambda_1AV_1 + c_2\\lambda_2AV_2 + ... c_n\\lambda_nAV_n \\\\\n",
    "                   & = & c_1\\lambda_1^2V_1 + c_2\\lambda_2^2V_2 + ... c_n\\lambda_n^2V_n \\\\\n",
    "                   & \\vdots & \\\\\n",
    "X^{(m)} = AX^{(m-1)} & = & c_1\\lambda_1^{m-1}AV_1 + c_2\\lambda_2^{m-1}AV_2 + ... c_n\\lambda_n^{m-1}AV_n \\\\\n",
    "                   & = & c_1\\lambda_1^mV_1 + c_2\\lambda_2^mV_2 + ... c_n\\lambda_n^mV_n \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "In the expression for $X^{(m)}$, we can then factor out $\\lambda_1^m$ to understand what happens as $m$ gets large.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(m)} =  \\lambda_1^m\\left(c_1V_1 + c_2\\left(\\frac{\\lambda_2}{\\lambda_1}\\right)^mV_2 + ... c_n\\left(\\frac{\\lambda_n}{\\lambda_1}\\right)^mV_n\\right) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If $|\\lambda_1| > |\\lambda_i|$ for all $i\\neq 1$, then $|\\lambda_i/\\lambda_1|< 1$ and $(\\lambda_i/\\lambda_1)^m$ will approach zero as $m$ gets large.  This means that if we repeatedly multiply a vector by the matrix $A$, eventually we will get a vector that is very nearly in the direction of the eigenvector that corresponds to the $\\lambda_1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate the calculation on the matrix shown here before we discuss the method further.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rrrr} -2 & 6 & 2 & -8 \\\\ -6 & 0 & 12 & 12 \\\\ -6 & 0 & 12 & 12 \\\\ -10 & 3 & 7 & 14 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As a matter of practicality, it is common to scale the vectors in the sequence to unit length as the Power Method is applied.  If the vectors in the sequence are not scaled, their magnitudes will grow if $\\lambda_1>1$ or decay if $\\lambda_1<1$.    Since all components of the vectors get divided by the same factor when the vector is scaled, this step doesn't change the ultimate behavior of the sequence.  The scaled sequence of vectors still approaches the direction of the eigenvector. \n",
    "\n",
    "We choose an arbitrary $X^{(0)}$ and calculate $X^{(20)}$ using the following rule.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(m)}=\\frac{AX^{(m-1)}}{||AX^{(m-1)}||}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.57523994e-12]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import laguide as lag\n",
    "A = np.array([[-2, 6, 2, -8],[-6, 0, 12, 12],[-6, 0, 12, 12],[-10, 3, 7, 14]])\n",
    "X = np.array([[1],[0],[0],[0]])\n",
    "\n",
    "m = 0\n",
    "while (m < 20):\n",
    "    X = A@X\n",
    "    X = X/lag.Magnitude(X)\n",
    "    m = m + 1\n",
    "    \n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if $X$ is the eigenvector of $A$ with unit magnitude, then $|AX| = |\\lambda_1X| = |\\lambda_1|$.  We can therefore approximate $|\\lambda_1|$ with $|AX|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.000000000020005\n"
     ]
    }
   ],
   "source": [
    "print(lag.Magnitude(A@X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that 24 is an estimate for $\\lambda_1$.   To determine if our calculation is correct, we can compare $AX$ with $\\lambda_1X$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.09570156e-11]\n",
      " [-9.45021839e-12]\n",
      " [-9.45021839e-12]\n",
      " [-1.57545088e-11]]\n"
     ]
    }
   ],
   "source": [
    "print(A@X - 24*X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the difference $AX-24X$ is small.  Note that in this case, we can even do the calculation with integer multiplication.  Notice that $X$ has 0 in the first entry and the other entries are equal.  If we set these entries to 1, the result is easy to calculate even without the aid of the computer.  (*Remember that we can change the magnitude of an eigenvector and it is still an eigenvector.*) \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "AX = \\left[ \\begin{array}{rrrr} -2 & 6 & 2 & -8 \\\\ -6 & 0 & 12 & 12 \\\\ -6 & 0 & 12 & 12 \\\\ -10 & 3 & 7 & 14 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{r} 0 \\\\ 1\\\\ 1 \\\\ 1 \\end{array}\\right] =\n",
    "\\left[ \\begin{array}{r} 0 \\\\ 24\\\\ 24 \\\\ 24 \\end{array}\\right] = 24X\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we do not know how many iterations we need to perform in order to get a good approximation of the eigenvector.  Instead we should specify a condition upon which we will be satisfied with the approximation and terminate the iteration.  For example, since $||AX^{(m)}||\\approx \\lambda_1$ and $AX^{(m)}\\approx \\lambda_1X^{(m)}$ we might require that $AX^{(m)} - ||AX^{(m)}||X^{(m)} < \\epsilon$ for some small number $\\epsilon$ known as a tolerance.  This condition ensures that $X^{(m)}$ functions roughly like an eigenvector.  It is also best to include in the code a limit on the number of iterations that will be carried out.  This ensures that the computation will eventually end, even if a satisfactory result has not yet been achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector is approximately:\n",
      "[[ 1.65181395e-06]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]] \n",
      "\n",
      "Magnitude of the eigenvalue is approximately:\n",
      "24.00002098082306 \n",
      "\n",
      "Magnitude of the difference is:\n",
      "4.3284704409694366e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[0],[0],[0]])\n",
    "\n",
    "m = 0\n",
    "tolerance = 0.0001\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "## Compute difference in stopping condition\n",
    "## Assign Y = AX to avoid computing AX multiple times\n",
    "Y = A@X\n",
    "difference = Y - lag.Magnitude(Y)*X\n",
    "\n",
    "while (m < MAX_ITERATIONS and lag.Magnitude(difference) > tolerance):\n",
    "    X = Y\n",
    "    X = X/lag.Magnitude(X)\n",
    "\n",
    "    ## Compute difference in stopping condition\n",
    "    Y = A@X\n",
    "    difference = Y - lag.Magnitude(Y)*X\n",
    "    \n",
    "    m = m + 1\n",
    "    \n",
    "print(\"Eigenvector is approximately:\")\n",
    "print(X,'\\n')\n",
    "print(\"Magnitude of the eigenvalue is approximately:\")\n",
    "print(lag.Magnitude(Y),'\\n')\n",
    "print(\"Magnitude of the difference is:\")\n",
    "print(lag.Magnitude(difference))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more common condition to require is that $||X^{(m)} - X^{(m-1})|| < \\epsilon$ for a given tolerance $\\epsilon$.  This condition merely requires that the vectors in the sequence get close to one another, not that they are actually approximate an eigenvector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector is approximately:\n",
      "[[ 2.64294012e-05]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]\n",
      " [-5.77350269e-01]] \n",
      "\n",
      "Magnitude of the eigenvalue is approximately:\n",
      "24.00002098082306 \n",
      "\n",
      "Magnitude of the difference is:\n",
      "8.434774776937697e-05\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1],[0],[0],[0]])\n",
    "\n",
    "m = 0\n",
    "tolerance = 0.0001\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "difference = X\n",
    "\n",
    "while (m < MAX_ITERATIONS and lag.Magnitude(difference) > tolerance):\n",
    "    X_previous = X\n",
    "    X = A@X\n",
    "    X = X/lag.Magnitude(X)\n",
    "\n",
    "    ## Compute difference in stopping condition\n",
    "    difference = X - X_previous\n",
    "    \n",
    "    m = m + 1\n",
    "    \n",
    "print(\"Eigenvector is approximately:\")\n",
    "print(X,'\\n')\n",
    "print(\"Magnitude of the eigenvalue is approximately:\")\n",
    "print(lag.Magnitude(Y),'\\n')\n",
    "print(\"Magnitude of the difference is:\")\n",
    "print(lag.Magnitude(difference))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Power Method is easy to understand and apply, it does have disadvantages.  The most apparent disadvantage is that the method only applies to the largest eigenvalue.  This is not a huge detriment since applications often only require an approximation of the largest eigenvalue.  Also, as we will demonstrate below, it is possible to easily modify the method to approximate the other eigenvalues.  A more significant disadvantage is that the rate at which the sequence converges can be slow in some circumstances.  For example, we can see that if $|\\lambda_1|$ is close to $|\\lambda_2|$, then $|\\lambda_1/\\lambda_2|^m$ approaches zero more slowly as $m$ gets large.  The Power Method may fail to converge at all if $|\\lambda_1| = |\\lambda_2|$, which occurs if $\\lambda_1 = -\\lambda_2$, or if $\\lambda_1$ and $\\lambda_2$ are a complex conjugate pair.  Additionally, the method may perform poorly if the $V_1$ component of $X^{(0)}$ is too small.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse power method (method 2)\n",
    "\n",
    "The **Inverse Power Method** is a modified version of the Power Method that allows us to approximate eigenvalues that are *not the largest*.  All that is needed to make the modification is two simple facts that relate changes in a matrix to changes in the eigenvalues of that matrix.  Let's suppose that $A$ is an invertible $n\\times n$ matrix with eigenvalue $\\lambda$ and corresponding eigenvector $V$, so that $AV=\\lambda V$.  If we multiply this equation by $A^{-1}$, we get $V=\\lambda A^{-1}V$, which can then be divided by $\\lambda$ to illustrate the useful fact.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A^{-1}V = \\frac{1}{\\lambda}V\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If $\\lambda$ is an eigenvalue of $A$, then $\\lambda^{-1}$ is an eigenvalue of $A^{-1}$.  Furthermore the eigenvector of $A$ is also an eigenvector of $A^{-1}$.  The important point here is that if $\\lambda_n$ is the smallest eigenvalue of $A$, then $\\lambda_n^{-1}$ is the *largest* eigenvector of $A^{-1}$.  If we want to approximate the smallest eigenvalue of $A$, we can just apply the Power Method to $A^{-1}$.\n",
    "\n",
    "We demonstrate the calculation for the following $3\\times 3$ matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rrrr} 9 & -1 & -3 \\\\ 0 & 6 & 0 \\\\ -6 & 3 & 6 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Again we choose an arbitrary $X^{(0)}$, and generate a sequence of vectors by multiplying by $A^{-1}$ and scaling the result to unit length.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(m)}=\\frac{A^{-1}X^{(m-1)}}{||A^{-1}X^{(m-1)}||}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector is approximately:\n",
      "[[-4.47193123e-01]\n",
      " [ 6.14168469e-05]\n",
      " [-8.94437425e-01]] \n",
      "\n",
      "Magnitude of the eigenvalue of A inverse is approximately:\n",
      "0.3333371476391265 \n",
      "\n",
      "Magnitude of the eigenvalue of A is approximately:\n",
      "2.999931351114087 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0],[1],[0]])\n",
    "\n",
    "m = 0\n",
    "tolerance = 0.0001\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "difference = X\n",
    "A = np.array([[9,-1,-3],[0,6,0],[-6,3,6]])\n",
    "A_inv = lag.Inverse(A)\n",
    "\n",
    "while (m < MAX_ITERATIONS and lag.Magnitude(difference) > tolerance):\n",
    "    X_previous = X\n",
    "    X = A_inv@X\n",
    "    X = X/lag.Magnitude(X)\n",
    "\n",
    "    ## Compute difference in stopping condition\n",
    "    difference = X - X_previous\n",
    "    \n",
    "    m = m + 1\n",
    "    \n",
    "print(\"Eigenvector is approximately:\")\n",
    "print(X,'\\n')\n",
    "print(\"Magnitude of the eigenvalue of A inverse is approximately:\")\n",
    "print(lag.Magnitude(A_inv@X),'\\n')\n",
    "print(\"Magnitude of the eigenvalue of A is approximately:\")\n",
    "print(lag.Magnitude(A@X),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact value of the smallest eigenvalue of $A$ is 3, which again can be verified by calculation.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "AV = \\left[ \\begin{array}{rrrr} 9 & -1 & -3 \\\\ 0 & 6 & 0 \\\\ -6 & 3 & 6 \\end{array}\\right]\n",
    "\\left[ \\begin{array}{r} 1 \\\\ 0\\\\ 2 \\end{array}\\right] =\n",
    "\\left[ \\begin{array}{r} 3 \\\\ 0 \\\\ 6 \\end{array}\\right] = 3V\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In our discussion of Inverse Matrices we noted that the construction of an inverse matrix is quite expensive since it requires the solution of $n$ systems of size $n\\times n$.  An alternative to constructing $A^{-1}$ and computing the  $X^{(m)}=A^{-1}X^{(m-1)}$ is to solve the system $AX^{(m)}=X^{(m-1)}$ to obtain $X^{(m)}$.  This means that we solve one $n\\times n$ system for every iteration.  This appears to require more work than the construction of $A^{-1}$, but in fact it is less since every system involves the same coefficient matrix.  We can therefore save much work by performing elimination only once and storing the result in an $LU$ factorization.  With the the matrix $A$ factored, each system $AX^{(m)}=X^{(m-1)}$ only requires one forward substitution and one backward substitution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector is approximately:\n",
      "[[-4.47193123e-01]\n",
      " [ 6.14168469e-05]\n",
      " [-8.94437425e-01]] \n",
      "\n",
      "Magnitude of the eigenvalue of A inverse is approximately:\n",
      "0.3333371476391265 \n",
      "\n",
      "Magnitude of the eigenvalue of A is approximately:\n",
      "2.999931351114087 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.linalg as sla\n",
    "\n",
    "X = np.array([[0],[1],[0]])\n",
    "\n",
    "m = 0\n",
    "tolerance = 0.0001\n",
    "MAX_ITERATIONS = 100\n",
    "\n",
    "difference = X\n",
    "A = np.array([[9,-1,-3],[0,6,0],[-6,3,6]])\n",
    "LU_factorization = sla.lu_factor(A)\n",
    "\n",
    "while (m < MAX_ITERATIONS and lag.Magnitude(difference) > tolerance):\n",
    "    X_previous = X\n",
    "    X = sla.lu_solve(LU_factorization,X)\n",
    "    X = X/lag.Magnitude(X)\n",
    "    difference = X - X_previous\n",
    "    m = m + 1\n",
    "  \n",
    "print(\"Eigenvector is approximately:\")\n",
    "print(X,'\\n')\n",
    "print(\"Magnitude of the eigenvalue of A inverse is approximately:\")\n",
    "print(lag.Magnitude(sla.lu_solve(LU_factorization,X)),'\\n')\n",
    "print(\"Magnitude of the eigenvalue of A is approximately:\")\n",
    "print(lag.Magnitude(A@X),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shifted inverse power method (method 3)\n",
    "\n",
    "Using a small modification to the Inverse Power Method, we can also approximate eigenvalues that are not the smallest.  For this variation of the method, we need to observe that if we \"shift\" the diagonal entries of a matrix by a scalar $\\mu$, all of the eigenvalues of the matrix are also shifted by $\\mu$.  Let $A$ be an $n\\times n$ matrix with eigenvalue $\\lambda$ and corresponding eigenvector $V$, so that $AV=\\lambda V$.  Then $(A-\\mu I)V = AV - \\mu V = \\lambda V - \\mu V = (\\lambda-\\mu)V$, which means that $V$ is also an eigenvector of the matrix $(A-\\mu I)$ corresponding to the eigenvalue $\\lambda -\\mu$.  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{1}{\\lambda_1-\\mu}, \\frac{1}{\\lambda_2-\\mu}, \\frac{1}{\\lambda_3-\\mu}, ....,\\frac{1}{\\lambda_n-\\mu} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This is useful because it allows us to now use the Inverse Power Method to approximate the eigenvalue of $A$ that lies closest to $\\mu$.  For example, if $\\mu$ is closest to $\\lambda_2$, then $|\\lambda_2-\\mu| < |\\lambda_i -\\mu|$ for all other $i\\neq 2$, which means that $(\\lambda_2-\\mu)$ can be approximated by applying the Inverse Power Method to $(A-\\mu I)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate the computation of the middle eigenvalue of the matrix from the previous example.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rrrr} 9 & -1 & -3 \\\\ 0 & 6 & 0 \\\\ -6 & 3 & 6 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "By using the Inverse Power Method we determined that the smallest eigenvalue of $A$ is 3.  Applying the Power Method directly will show that the largest eigenvalue of $A$ is 12.  Since the third eigenvalue must lie somewhere in between these extremes, we choose $\\mu$ to be exactly in the middle at $7.5$.  Note that once we have a good approximation to the eigenvector with $X^{(m)}$, we can approximate the eigenvalue of $A$ with $||AX^{(m)}||$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvector is approximately:\n",
      "[[0.44232587]\n",
      " [0.88465174]\n",
      " [0.14744196]] \n",
      "\n",
      "Eigenvalue of A is approximately:\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0],[1],[0]])\n",
    "\n",
    "m = 0\n",
    "tolerance = 0.0001\n",
    "MAX_ITERATIONS = 100\n",
    "difference = X\n",
    "\n",
    "A = np.array([[9,-1,-3],[0,6,0],[-6,3,6]])\n",
    "I = np.eye(3)\n",
    "mu = 7.5\n",
    "Shifted_A = A-mu*I\n",
    "LU_factorization = sla.lu_factor(Shifted_A)\n",
    "\n",
    "while (m < MAX_ITERATIONS and lag.Magnitude(difference) > tolerance):\n",
    "    X_previous = X\n",
    "    X = sla.lu_solve(LU_factorization,X)\n",
    "    X = X/lag.Magnitude(X)\n",
    "\n",
    "    ## Compute difference in stopping condition\n",
    "    difference = X - X_previous\n",
    "    \n",
    "    m = m + 1\n",
    "\n",
    "print(\"Eigenvector is approximately:\")\n",
    "print(X,'\\n')\n",
    "print(\"Eigenvalue of A is approximately:\")\n",
    "print(lag.Magnitude(A@X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 1:** Let $A$ be the matrix from the Inverse Power Method example.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rrr} 9 & -1 & -3 \\\\ 0 & 6 & 0 \\\\ -6 & 3 & 6 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "($a$) Use the Power Method to approximate the largest eigenvalue $\\lambda_1$.  Verify that the exact value of $\\lambda_1$ is 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($b$) Construct the line through $0$ and $Ax_k,$ that is, graph the iterations of the eigenvectors. What happens as $k$ increases? (hint: see Figure 1 of Lay page 322)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($c$) Apply the Inverse Power Method with a shift of $\\mu = 10$.  Explain why the results differ from those in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($d$) Apply the Inverse Power Method with a shift of $\\mu = 7.5$ and the initial vector given below.  Explain why the sequence of vectors approach the eigenvector corresponding to $\\lambda_1$\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X^{(0)} = \\left[ \\begin{array}{r} 1 \\\\ 0  \\\\ 0 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Let $B$ be the following matrix.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "B = \\left[ \\begin{array}{rrrr} -2 & -18 & 6 \\\\ -11 & 3 & 11 \\\\ -27 & 15 & 31 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "($a$) Apply the Power Method and Inverse Power Method with shifts to approximate all eigenvalues of the matrix $B$. (*Note that one of the eigenvalues of this matrix is negative.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($b$) Check your results using the $\\texttt{eig}$ function in SciPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications: Discrete Dynamical Systems\n",
    "\n",
    "In this second section, we recall that eigenvalues and eigenvectors provide the key to understanding the long-term behavior, or evolution, of a dynamical system described by a difference equation $x_{k+1} = Ax_k.$ Read Lay, section 5.6 for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Produce the general solution of the dynamical system $x_{k+1}=Ax_k$ when $A$ is the stochastic matrix for the Hertz Rent-A-Car model in Denver. Assume that the company has a fleet of about 2000 cars. The pattern of rental and return locations is given by the factions in the table below. On a typical day, about how many cars will be rented or ready to rent from the downtown location where\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A = \\left[ \\begin{array}{rrrr} .90 & .01 & .09 \\\\ .01 & .90 & .01 \\\\ .09 & .09 & .90 \\end{array}\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where columns represent cars rented from DIA, downtown, local airport, respectively, and the rows represent cars returned to DIA, downtown, local airport, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Construct a stage-matrix model for an animal species that has two life stages: juvenile (up to 1 year old) and adult. Suppose the female adults give birth each year to an average of 1.6 female juveniles. Each year, 30% of the juveniles survive to become adults and 80% of the adults survive. For $k\\geq 0,$ let $x_k=(j_k, a_k),$ where the entries in $x_k$ are the numbersof female juveniles and female adults in year $k.$\n",
    "\n",
    "($a$) Construct the stage-matrix $A$ such that $x_{k+1} = Ax_k$ for $k \\geq 0.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($b$)  Show that the population is growing, compute the even- tual growth rate of the population, and give the eventual ratio of juveniles to adults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($c$)  Suppose that initially there are 15 juveniles and 10 adults in the population. Produce four graphs that show how the population changes over eight years: (1) the number of juveniles, (2) the number of adults, (3) the total population, and (4) the ratio of juveniles to adults (each year). Further, when does the ratio in (4) seem to stabilize? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** A herd of American buffalo (bison) can be modeled by a stage matrix similar to that for the spotted owls. The females can be divided into calves (up to 1 year old), yearlings (1 to 2 years), and adults. Suppose an average of 42 female calves are born each year per 100 adult females. (Only adults produce offspring.) Each year, about 60% of the calves survive, 75% of the yearlings survive, and 95% of the adults survive. For $k\\geq 0,$ let $x_k=(c_k, y_k, a_k)$, where the entries in $x_k$ are the numbers of females in each life stage at year $k.$\n",
    "\n",
    "\n",
    "($a$)  Construct the stage-matrix $A$ for the buffalo herd, such that $x_{k+1}=Ax_k$ for $k\\geq 0.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "($b$)  Show that the buffalo herd is growing, determine the expected growth rate after many years, and give the expected numbers of calves and yearlings present per 100 adults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Discussion: \n",
    "\n",
    "Please provide a 250-300 word report on what you learned from this project. Provide any more details about the project and expand on your favorite part of the project. Include any other information you have about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- Burden, Richard L. et al. *Numerical Analysis*. 10th ed., Cengage Learning, 2014.\n",
    "- Golub, Gene H. and Charles F. Van Loan. *Matrix Computations*., The Johns Hopkins University Press, 1989.\n",
    "- Lay, David C. et al. *Linear Algebra and its Applications*., 5th ed., Pearson, 2016. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
